{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "469eccff-f2d9-47e6-b2e7-86b8401d8d9b",
   "metadata": {
    "id": "469eccff-f2d9-47e6-b2e7-86b8401d8d9b"
   },
   "source": [
    "<div>\n",
    "<img src=https://www.institutedata.com/wp-content/uploads/2019/10/iod_h_tp_primary_c.svg width=\"300\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a81de7-1bfa-4e29-98d8-e98162bb7612",
   "metadata": {
    "id": "57a81de7-1bfa-4e29-98d8-e98162bb7612"
   },
   "source": [
    "# Lab 8.5 - Prompting Large Language Models\n",
    "\n",
    "In this lab we will practise prompting with a few Large Language Models (LLMs) using Groq (not to be confused with Grok). Groq is a platform that provides access to their custom-built AI hardware via APIs, allowing users to run open-source models such as Llama.\n",
    "\n",
    "We shall see that while LLMs are powerful tools, how you ask a question or frame a task can dramatically influence the results obtained."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca332d1-638f-44c8-9f20-eab2f62bec2a",
   "metadata": {
    "id": "7ca332d1-638f-44c8-9f20-eab2f62bec2a"
   },
   "source": [
    "## Set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111d6485-4dd7-47d7-a0f4-eba2a6cb3719",
   "metadata": {
    "id": "111d6485-4dd7-47d7-a0f4-eba2a6cb3719"
   },
   "source": [
    "Step 1: Sign up for a free Groq account at https://console.groq.com/home .\n",
    "\n",
    "Step 2: Create a new API key at https://console.groq.com/keys. Copy-paste it into an empty text file called 'groq_key.txt'.\n",
    "\n",
    "Running the next cell will then read in this key and assign it to the variable `groq_key`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10654a90-f382-4a04-b2e5-dc01fc5c0916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: groq in /Users/Anaconda3/anaconda3/lib/python3.12/site-packages (0.29.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/Anaconda3/anaconda3/lib/python3.12/site-packages (from groq) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/Anaconda3/anaconda3/lib/python3.12/site-packages (from groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/Anaconda3/anaconda3/lib/python3.12/site-packages (from groq) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/Anaconda3/anaconda3/lib/python3.12/site-packages (from groq) (2.8.2)\n",
      "Requirement already satisfied: sniffio in /Users/Anaconda3/anaconda3/lib/python3.12/site-packages (from groq) (1.3.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in /Users/Anaconda3/anaconda3/lib/python3.12/site-packages (from groq) (4.11.0)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/Anaconda3/anaconda3/lib/python3.12/site-packages (from anyio<5,>=3.5.0->groq) (3.7)\n",
      "Requirement already satisfied: certifi in /Users/Anaconda3/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->groq) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/Anaconda3/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->groq) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/Anaconda3/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/Anaconda3/anaconda3/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->groq) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /Users/Anaconda3/anaconda3/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->groq) (2.20.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install groq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56441b82-ddc5-46e9-9583-878e8a807b7f",
   "metadata": {
    "id": "56441b82-ddc5-46e9-9583-878e8a807b7f"
   },
   "outputs": [],
   "source": [
    "groqfilename = 'groq_key.txt'\n",
    "\n",
    "try:\n",
    "    with open(groqfilename, 'r') as f:\n",
    "        groq_key = f.read().strip()\n",
    "except FileNotFoundError:\n",
    "    print(f\"'{groqfilename}' file not found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd0b4dfc-47a9-46f9-ae1a-6241b07ba41c",
   "metadata": {
    "id": "dd0b4dfc-47a9-46f9-ae1a-6241b07ba41c"
   },
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "import requests\n",
    "import pandas as pd\n",
    "from IPython.display import Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "606b6102-4a13-44bb-9324-d1f9aff8ac88",
   "metadata": {
    "id": "606b6102-4a13-44bb-9324-d1f9aff8ac88"
   },
   "outputs": [],
   "source": [
    "#!pip install groq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eef37e2-24ba-4e6d-a8ce-87ade1ef1227",
   "metadata": {
    "id": "1eef37e2-24ba-4e6d-a8ce-87ade1ef1227"
   },
   "source": [
    "First create an instance of the Groq client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54fec468-0ae0-42e1-ab8c-087b97c9818b",
   "metadata": {
    "id": "54fec468-0ae0-42e1-ab8c-087b97c9818b"
   },
   "outputs": [],
   "source": [
    "client = Groq(api_key=groq_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a097cd77-7a8d-4502-ae6e-607782706b26",
   "metadata": {
    "id": "a097cd77-7a8d-4502-ae6e-607782706b26"
   },
   "source": [
    "The following code shows what models are currently accessible through Groq. `context_window` refers to the size of memory (in tokens) during a session and `max_completion_tokens` is the maximum number of tokens that are generated in an output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc7683cb-580e-45c6-a52c-4d9aef8b3ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'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'\n"
     ]
    }
   ],
   "source": [
    "print(repr(groq_key))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "33fc8058-c9c7-49b3-b294-cb4e12de20f4",
   "metadata": {
    "id": "33fc8058-c9c7-49b3-b294-cb4e12de20f4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>object</th>\n",
       "      <th>created</th>\n",
       "      <th>owned_by</th>\n",
       "      <th>active</th>\n",
       "      <th>context_window</th>\n",
       "      <th>public_apps</th>\n",
       "      <th>max_completion_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>meta-llama/llama-prompt-guard-2-86m</td>\n",
       "      <td>model</td>\n",
       "      <td>1748632165</td>\n",
       "      <td>Meta</td>\n",
       "      <td>True</td>\n",
       "      <td>512</td>\n",
       "      <td>None</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>meta-llama/llama-prompt-guard-2-22m</td>\n",
       "      <td>model</td>\n",
       "      <td>1748632101</td>\n",
       "      <td>Meta</td>\n",
       "      <td>True</td>\n",
       "      <td>512</td>\n",
       "      <td>None</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>qwen/qwen3-32b</td>\n",
       "      <td>model</td>\n",
       "      <td>1748396646</td>\n",
       "      <td>Alibaba Cloud</td>\n",
       "      <td>True</td>\n",
       "      <td>131072</td>\n",
       "      <td>None</td>\n",
       "      <td>40960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>meta-llama/llama-guard-4-12b</td>\n",
       "      <td>model</td>\n",
       "      <td>1746743847</td>\n",
       "      <td>Meta</td>\n",
       "      <td>True</td>\n",
       "      <td>131072</td>\n",
       "      <td>None</td>\n",
       "      <td>1024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>meta-llama/llama-4-maverick-17b-128e-instruct</td>\n",
       "      <td>model</td>\n",
       "      <td>1743877158</td>\n",
       "      <td>Meta</td>\n",
       "      <td>True</td>\n",
       "      <td>131072</td>\n",
       "      <td>None</td>\n",
       "      <td>8192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>meta-llama/llama-4-scout-17b-16e-instruct</td>\n",
       "      <td>model</td>\n",
       "      <td>1743874824</td>\n",
       "      <td>Meta</td>\n",
       "      <td>True</td>\n",
       "      <td>131072</td>\n",
       "      <td>None</td>\n",
       "      <td>8192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>compound-beta-mini</td>\n",
       "      <td>model</td>\n",
       "      <td>1742953279</td>\n",
       "      <td>Groq</td>\n",
       "      <td>True</td>\n",
       "      <td>131072</td>\n",
       "      <td>None</td>\n",
       "      <td>8192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>qwen-qwq-32b</td>\n",
       "      <td>model</td>\n",
       "      <td>1741214760</td>\n",
       "      <td>Alibaba Cloud</td>\n",
       "      <td>True</td>\n",
       "      <td>131072</td>\n",
       "      <td>None</td>\n",
       "      <td>131072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>compound-beta</td>\n",
       "      <td>model</td>\n",
       "      <td>1740880017</td>\n",
       "      <td>Groq</td>\n",
       "      <td>True</td>\n",
       "      <td>131072</td>\n",
       "      <td>None</td>\n",
       "      <td>8192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>playai-tts-arabic</td>\n",
       "      <td>model</td>\n",
       "      <td>1740682783</td>\n",
       "      <td>PlayAI</td>\n",
       "      <td>True</td>\n",
       "      <td>8192</td>\n",
       "      <td>None</td>\n",
       "      <td>8192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>playai-tts</td>\n",
       "      <td>model</td>\n",
       "      <td>1740682771</td>\n",
       "      <td>PlayAI</td>\n",
       "      <td>True</td>\n",
       "      <td>8192</td>\n",
       "      <td>None</td>\n",
       "      <td>8192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>mistral-saba-24b</td>\n",
       "      <td>model</td>\n",
       "      <td>1739996492</td>\n",
       "      <td>Mistral AI</td>\n",
       "      <td>True</td>\n",
       "      <td>32768</td>\n",
       "      <td>None</td>\n",
       "      <td>32768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>deepseek-r1-distill-llama-70b</td>\n",
       "      <td>model</td>\n",
       "      <td>1737924940</td>\n",
       "      <td>DeepSeek / Meta</td>\n",
       "      <td>True</td>\n",
       "      <td>131072</td>\n",
       "      <td>None</td>\n",
       "      <td>131072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>allam-2-7b</td>\n",
       "      <td>model</td>\n",
       "      <td>1737672203</td>\n",
       "      <td>SDAIA</td>\n",
       "      <td>True</td>\n",
       "      <td>4096</td>\n",
       "      <td>None</td>\n",
       "      <td>4096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>llama-3.3-70b-versatile</td>\n",
       "      <td>model</td>\n",
       "      <td>1733447754</td>\n",
       "      <td>Meta</td>\n",
       "      <td>True</td>\n",
       "      <td>131072</td>\n",
       "      <td>None</td>\n",
       "      <td>32768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>whisper-large-v3-turbo</td>\n",
       "      <td>model</td>\n",
       "      <td>1728413088</td>\n",
       "      <td>OpenAI</td>\n",
       "      <td>True</td>\n",
       "      <td>448</td>\n",
       "      <td>None</td>\n",
       "      <td>448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>llama-3.1-8b-instant</td>\n",
       "      <td>model</td>\n",
       "      <td>1693721698</td>\n",
       "      <td>Meta</td>\n",
       "      <td>True</td>\n",
       "      <td>131072</td>\n",
       "      <td>None</td>\n",
       "      <td>131072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>distil-whisper-large-v3-en</td>\n",
       "      <td>model</td>\n",
       "      <td>1693721698</td>\n",
       "      <td>Hugging Face</td>\n",
       "      <td>True</td>\n",
       "      <td>448</td>\n",
       "      <td>None</td>\n",
       "      <td>448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>llama3-70b-8192</td>\n",
       "      <td>model</td>\n",
       "      <td>1693721698</td>\n",
       "      <td>Meta</td>\n",
       "      <td>True</td>\n",
       "      <td>8192</td>\n",
       "      <td>None</td>\n",
       "      <td>8192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>whisper-large-v3</td>\n",
       "      <td>model</td>\n",
       "      <td>1693721698</td>\n",
       "      <td>OpenAI</td>\n",
       "      <td>True</td>\n",
       "      <td>448</td>\n",
       "      <td>None</td>\n",
       "      <td>448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gemma2-9b-it</td>\n",
       "      <td>model</td>\n",
       "      <td>1693721698</td>\n",
       "      <td>Google</td>\n",
       "      <td>True</td>\n",
       "      <td>8192</td>\n",
       "      <td>None</td>\n",
       "      <td>8192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>llama3-8b-8192</td>\n",
       "      <td>model</td>\n",
       "      <td>1693721698</td>\n",
       "      <td>Meta</td>\n",
       "      <td>True</td>\n",
       "      <td>8192</td>\n",
       "      <td>None</td>\n",
       "      <td>8192</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               id object     created  \\\n",
       "14            meta-llama/llama-prompt-guard-2-86m  model  1748632165   \n",
       "19            meta-llama/llama-prompt-guard-2-22m  model  1748632101   \n",
       "4                                  qwen/qwen3-32b  model  1748396646   \n",
       "11                   meta-llama/llama-guard-4-12b  model  1746743847   \n",
       "9   meta-llama/llama-4-maverick-17b-128e-instruct  model  1743877158   \n",
       "1       meta-llama/llama-4-scout-17b-16e-instruct  model  1743874824   \n",
       "15                             compound-beta-mini  model  1742953279   \n",
       "16                                   qwen-qwq-32b  model  1741214760   \n",
       "20                                  compound-beta  model  1740880017   \n",
       "17                              playai-tts-arabic  model  1740682783   \n",
       "12                                     playai-tts  model  1740682771   \n",
       "7                                mistral-saba-24b  model  1739996492   \n",
       "10                  deepseek-r1-distill-llama-70b  model  1737924940   \n",
       "21                                     allam-2-7b  model  1737672203   \n",
       "3                         llama-3.3-70b-versatile  model  1733447754   \n",
       "5                          whisper-large-v3-turbo  model  1728413088   \n",
       "13                           llama-3.1-8b-instant  model  1693721698   \n",
       "8                      distil-whisper-large-v3-en  model  1693721698   \n",
       "6                                 llama3-70b-8192  model  1693721698   \n",
       "18                               whisper-large-v3  model  1693721698   \n",
       "2                                    gemma2-9b-it  model  1693721698   \n",
       "0                                  llama3-8b-8192  model  1693721698   \n",
       "\n",
       "           owned_by  active  context_window public_apps  max_completion_tokens  \n",
       "14             Meta    True             512        None                    512  \n",
       "19             Meta    True             512        None                    512  \n",
       "4     Alibaba Cloud    True          131072        None                  40960  \n",
       "11             Meta    True          131072        None                   1024  \n",
       "9              Meta    True          131072        None                   8192  \n",
       "1              Meta    True          131072        None                   8192  \n",
       "15             Groq    True          131072        None                   8192  \n",
       "16    Alibaba Cloud    True          131072        None                 131072  \n",
       "20             Groq    True          131072        None                   8192  \n",
       "17           PlayAI    True            8192        None                   8192  \n",
       "12           PlayAI    True            8192        None                   8192  \n",
       "7        Mistral AI    True           32768        None                  32768  \n",
       "10  DeepSeek / Meta    True          131072        None                 131072  \n",
       "21            SDAIA    True            4096        None                   4096  \n",
       "3              Meta    True          131072        None                  32768  \n",
       "5            OpenAI    True             448        None                    448  \n",
       "13             Meta    True          131072        None                 131072  \n",
       "8      Hugging Face    True             448        None                    448  \n",
       "6              Meta    True            8192        None                   8192  \n",
       "18           OpenAI    True             448        None                    448  \n",
       "2            Google    True            8192        None                   8192  \n",
       "0              Meta    True            8192        None                   8192  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://api.groq.com/openai/v1/models\"\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {groq_key}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "pd.DataFrame(response.json()['data']).sort_values(['created'], ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b438489-c102-434b-872d-e807169deef6",
   "metadata": {
    "id": "2b438489-c102-434b-872d-e807169deef6"
   },
   "source": [
    "The Groq client object enables interaction with the Groq REST API and a chat completion request is made via the client.chat.completions.create method.\n",
    "\n",
    "The most important arguments of the client.chat.completions.create method are the following:\n",
    "* messages: a list of messages (dictionary form) that make up the conversation to date\n",
    "* model: a string indicating which model to use (see [list of models](https://console.groq.com/docs/models))\n",
    "* max_completion_tokens: the maximum number of tokens that are generated in the chat completion\n",
    "* response_format: setting this to `{ \"type\": \"json_object\" }` enables JSON output\n",
    "* seed: sample deterministically as best as possible, though identical outputs each time are not guaranteed\n",
    "* temperature: between 0 and 2 where higher values like 0.8 make the output more random (creative) and values like 0.2 are more focused and deterministic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "99216392-acc3-4a00-826b-c166a1e52534",
   "metadata": {
    "id": "99216392-acc3-4a00-826b-c166a1e52534"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method create in module groq.resources.chat.completions:\n",
      "\n",
      "create(*, messages: 'Iterable[ChatCompletionMessageParam]', model: \"Union[str, Literal['gemma2-9b-it', 'llama-3.3-70b-versatile', 'llama-3.1-8b-instant', 'llama-guard-3-8b', 'llama3-70b-8192', 'llama3-8b-8192']]\", exclude_domains: 'Optional[List[str]] | NotGiven' = NOT_GIVEN, frequency_penalty: 'Optional[float] | NotGiven' = NOT_GIVEN, function_call: 'Optional[completion_create_params.FunctionCall] | NotGiven' = NOT_GIVEN, functions: 'Optional[Iterable[completion_create_params.Function]] | NotGiven' = NOT_GIVEN, include_domains: 'Optional[List[str]] | NotGiven' = NOT_GIVEN, logit_bias: 'Optional[Dict[str, int]] | NotGiven' = NOT_GIVEN, logprobs: 'Optional[bool] | NotGiven' = NOT_GIVEN, max_completion_tokens: 'Optional[int] | NotGiven' = NOT_GIVEN, max_tokens: 'Optional[int] | NotGiven' = NOT_GIVEN, metadata: 'Optional[Dict[str, str]] | NotGiven' = NOT_GIVEN, n: 'Optional[int] | NotGiven' = NOT_GIVEN, parallel_tool_calls: 'Optional[bool] | NotGiven' = NOT_GIVEN, presence_penalty: 'Optional[float] | NotGiven' = NOT_GIVEN, reasoning_effort: \"Optional[Literal['none', 'default']] | NotGiven\" = NOT_GIVEN, reasoning_format: \"Optional[Literal['hidden', 'raw', 'parsed']] | NotGiven\" = NOT_GIVEN, response_format: 'Optional[completion_create_params.ResponseFormat] | NotGiven' = NOT_GIVEN, search_settings: 'Optional[completion_create_params.SearchSettings] | NotGiven' = NOT_GIVEN, seed: 'Optional[int] | NotGiven' = NOT_GIVEN, service_tier: \"Optional[Literal['auto', 'on_demand', 'flex']] | NotGiven\" = NOT_GIVEN, stop: 'Union[Optional[str], List[str], None] | NotGiven' = NOT_GIVEN, store: 'Optional[bool] | NotGiven' = NOT_GIVEN, stream: 'Optional[Literal[False]] | Literal[True] | NotGiven' = NOT_GIVEN, temperature: 'Optional[float] | NotGiven' = NOT_GIVEN, tool_choice: 'Optional[ChatCompletionToolChoiceOptionParam] | NotGiven' = NOT_GIVEN, tools: 'Optional[Iterable[ChatCompletionToolParam]] | NotGiven' = NOT_GIVEN, top_logprobs: 'Optional[int] | NotGiven' = NOT_GIVEN, top_p: 'Optional[float] | NotGiven' = NOT_GIVEN, user: 'Optional[str] | NotGiven' = NOT_GIVEN, extra_headers: 'Headers | None' = None, extra_query: 'Query | None' = None, extra_body: 'Body | None' = None, timeout: 'float | httpx.Timeout | None | NotGiven' = NOT_GIVEN) -> 'ChatCompletion | Stream[ChatCompletionChunk]' method of groq.resources.chat.completions.Completions instance\n",
      "    Creates a model response for the given chat conversation.\n",
      "\n",
      "    Args:\n",
      "      messages: A list of messages comprising the conversation so far.\n",
      "\n",
      "      model: ID of the model to use. For details on which models are compatible with the Chat\n",
      "          API, see available [models](https://console.groq.com/docs/models)\n",
      "\n",
      "      exclude_domains: Deprecated: Use search_settings.exclude_domains instead. A list of domains to\n",
      "          exclude from the search results when the model uses a web search tool.\n",
      "\n",
      "      frequency_penalty: Number between -2.0 and 2.0. Positive values penalize new tokens based on their\n",
      "          existing frequency in the text so far, decreasing the model's likelihood to\n",
      "          repeat the same line verbatim.\n",
      "\n",
      "      function_call: Deprecated in favor of `tool_choice`.\n",
      "\n",
      "          Controls which (if any) function is called by the model. `none` means the model\n",
      "          will not call a function and instead generates a message. `auto` means the model\n",
      "          can pick between generating a message or calling a function. Specifying a\n",
      "          particular function via `{\"name\": \"my_function\"}` forces the model to call that\n",
      "          function.\n",
      "\n",
      "          `none` is the default when no functions are present. `auto` is the default if\n",
      "          functions are present.\n",
      "\n",
      "      functions: Deprecated in favor of `tools`.\n",
      "\n",
      "          A list of functions the model may generate JSON inputs for.\n",
      "\n",
      "      include_domains: Deprecated: Use search_settings.include_domains instead. A list of domains to\n",
      "          include in the search results when the model uses a web search tool.\n",
      "\n",
      "      logit_bias: This is not yet supported by any of our models. Modify the likelihood of\n",
      "          specified tokens appearing in the completion.\n",
      "\n",
      "      logprobs: This is not yet supported by any of our models. Whether to return log\n",
      "          probabilities of the output tokens or not. If true, returns the log\n",
      "          probabilities of each output token returned in the `content` of `message`.\n",
      "\n",
      "      max_completion_tokens: The maximum number of tokens that can be generated in the chat completion. The\n",
      "          total length of input tokens and generated tokens is limited by the model's\n",
      "          context length.\n",
      "\n",
      "      max_tokens: Deprecated in favor of `max_completion_tokens`. The maximum number of tokens\n",
      "          that can be generated in the chat completion. The total length of input tokens\n",
      "          and generated tokens is limited by the model's context length.\n",
      "\n",
      "      metadata: This parameter is not currently supported.\n",
      "\n",
      "      n: How many chat completion choices to generate for each input message. Note that\n",
      "          the current moment, only n=1 is supported. Other values will result in a 400\n",
      "          response.\n",
      "\n",
      "      parallel_tool_calls: Whether to enable parallel function calling during tool use.\n",
      "\n",
      "      presence_penalty: Number between -2.0 and 2.0. Positive values penalize new tokens based on\n",
      "          whether they appear in the text so far, increasing the model's likelihood to\n",
      "          talk about new topics.\n",
      "\n",
      "      reasoning_effort: this field is only available for qwen3 models. Set to 'none' to disable\n",
      "          reasoning. Set to 'default' or null to let Qwen reason.\n",
      "\n",
      "      reasoning_format: Specifies how to output reasoning tokens\n",
      "\n",
      "      response_format: An object specifying the format that the model must output. Setting to\n",
      "          `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured Outputs\n",
      "          which ensures the model will match your supplied JSON schema. json_schema\n",
      "          response format is only supported on llama 4 models. Setting to\n",
      "          `{ \"type\": \"json_object\" }` enables the older JSON mode, which ensures the\n",
      "          message the model generates is valid JSON. Using `json_schema` is preferred for\n",
      "          models that support it.\n",
      "\n",
      "      search_settings: Settings for web search functionality when the model uses a web search tool.\n",
      "\n",
      "      seed: If specified, our system will make a best effort to sample deterministically,\n",
      "          such that repeated requests with the same `seed` and parameters should return\n",
      "          the same result. Determinism is not guaranteed, and you should refer to the\n",
      "          `system_fingerprint` response parameter to monitor changes in the backend.\n",
      "\n",
      "      service_tier: The service tier to use for the request. Defaults to `on_demand`.\n",
      "\n",
      "          - `auto` will automatically select the highest tier available within the rate\n",
      "            limits of your organization.\n",
      "          - `flex` uses the flex tier, which will succeed or fail quickly.\n",
      "\n",
      "      stop: Up to 4 sequences where the API will stop generating further tokens. The\n",
      "          returned text will not contain the stop sequence.\n",
      "\n",
      "      store: This parameter is not currently supported.\n",
      "\n",
      "      stream: If set, partial message deltas will be sent. Tokens will be sent as data-only\n",
      "          [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format)\n",
      "          as they become available, with the stream terminated by a `data: [DONE]`\n",
      "          message. [Example code](/docs/text-chat#streaming-a-chat-completion).\n",
      "\n",
      "      temperature: What sampling temperature to use, between 0 and 2. Higher values like 0.8 will\n",
      "          make the output more random, while lower values like 0.2 will make it more\n",
      "          focused and deterministic. We generally recommend altering this or top_p but not\n",
      "          both.\n",
      "\n",
      "      tool_choice: Controls which (if any) tool is called by the model. `none` means the model will\n",
      "          not call any tool and instead generates a message. `auto` means the model can\n",
      "          pick between generating a message or calling one or more tools. `required` means\n",
      "          the model must call one or more tools. Specifying a particular tool via\n",
      "          `{\"type\": \"function\", \"function\": {\"name\": \"my_function\"}}` forces the model to\n",
      "          call that tool.\n",
      "\n",
      "          `none` is the default when no tools are present. `auto` is the default if tools\n",
      "          are present.\n",
      "\n",
      "      tools: A list of tools the model may call. Currently, only functions are supported as a\n",
      "          tool. Use this to provide a list of functions the model may generate JSON inputs\n",
      "          for. A max of 128 functions are supported.\n",
      "\n",
      "      top_logprobs: This is not yet supported by any of our models. An integer between 0 and 20\n",
      "          specifying the number of most likely tokens to return at each token position,\n",
      "          each with an associated log probability. `logprobs` must be set to `true` if\n",
      "          this parameter is used.\n",
      "\n",
      "      top_p: An alternative to sampling with temperature, called nucleus sampling, where the\n",
      "          model considers the results of the tokens with top_p probability mass. So 0.1\n",
      "          means only the tokens comprising the top 10% probability mass are considered. We\n",
      "          generally recommend altering this or temperature but not both.\n",
      "\n",
      "      user: A unique identifier representing your end-user, which can help us monitor and\n",
      "          detect abuse.\n",
      "\n",
      "      extra_headers: Send extra headers\n",
      "\n",
      "      extra_query: Add additional query parameters to the request\n",
      "\n",
      "      extra_body: Add additional JSON properties to the request\n",
      "\n",
      "      timeout: Override the client-level default timeout for this request, in seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(client.chat.completions.create)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d17d3c7-d7b5-47ed-b514-40b0a3e577b7",
   "metadata": {
    "id": "4d17d3c7-d7b5-47ed-b514-40b0a3e577b7"
   },
   "source": [
    "As a first example, note how the messages input is given as a list of a dictionaries with `role` and `content` keys. This is in a ChatML format recognised by many LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9cfd33fb-15fc-409d-a5a9-e8770885df81",
   "metadata": {
    "id": "9cfd33fb-15fc-409d-a5a9-e8770885df81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large language models are artificial intelligence (AI) systems that process and generate human-like language. They work by:\n",
      "\n",
      "1. **Training on vast amounts of text data**: The models are trained on massive datasets of text from various sources, such as books, articles, and conversations.\n",
      "2. **Learning patterns and relationships**: Through this training, the models learn to recognize patterns and relationships between words, phrases, and ideas in language.\n",
      "3. **Using neural networks**: The models use complex neural networks to analyze and generate text, allowing them to predict the next word or character in a sequence.\n",
      "4. **Generating text based on context**: When given a prompt or input, the model uses its learned patterns and relationships to generate text that is coherent and contextually relevant.\n",
      "\n",
      "This enables large language models to perform tasks like answering questions, translating text, and even creating original content, such as stories or dialogue.\n"
     ]
    }
   ],
   "source": [
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {   \"role\": \"system\", # sets the persona of the model\n",
    "            \"content\": \"You are a helpful assistant.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\", # what the user wants the assistant to do\n",
    "            \"content\": \"Explain briefly how large language models work\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728b60c2-4300-4338-b325-c3ea876c1afe",
   "metadata": {
    "id": "728b60c2-4300-4338-b325-c3ea876c1afe"
   },
   "source": [
    "The output is in Markdown format so the following line formats this text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "88845bb3-4a3e-403a-93ab-439c46aa1832",
   "metadata": {
    "id": "88845bb3-4a3e-403a-93ab-439c46aa1832"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Large language models are artificial intelligence (AI) systems that process and generate human-like language. They work by:\n",
       "\n",
       "1. **Training on vast amounts of text data**: The models are trained on massive datasets of text from various sources, such as books, articles, and conversations.\n",
       "2. **Learning patterns and relationships**: Through this training, the models learn to recognize patterns and relationships between words, phrases, and ideas in language.\n",
       "3. **Using neural networks**: The models use complex neural networks to analyze and generate text, allowing them to predict the next word or character in a sequence.\n",
       "4. **Generating text based on context**: When given a prompt or input, the model uses its learned patterns and relationships to generate text that is coherent and contextually relevant.\n",
       "\n",
       "This enables large language models to perform tasks like answering questions, translating text, and even creating original content, such as stories or dialogue."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c8c97b-1711-4355-b108-86bed769c109",
   "metadata": {
    "id": "e1c8c97b-1711-4355-b108-86bed769c109"
   },
   "source": [
    "## Text summarisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930cdd6b-8d44-4b3e-b161-0ebcd4600bc4",
   "metadata": {
    "id": "930cdd6b-8d44-4b3e-b161-0ebcd4600bc4"
   },
   "source": [
    "We start with a llama3-8b-8192, a model using just over 8 billion parameters with at most 8192 tokens produced as output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b56002-d85b-44d6-a2d0-4b1513eeb4f2",
   "metadata": {
    "id": "b7b56002-d85b-44d6-a2d0-4b1513eeb4f2"
   },
   "source": [
    "Here is an article to be summarised from the [cnn_dailymail](https://huggingface.co/datasets/cnn_dailymail) dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7c426f9f-c780-4ca6-b913-9b5c63b8bb29",
   "metadata": {
    "id": "7c426f9f-c780-4ca6-b913-9b5c63b8bb29"
   },
   "outputs": [],
   "source": [
    "story = \"\"\"\n",
    "SAN FRANCISCO, California (CNN) -- A magnitude 4.2 earthquake shook the San Francisco area Friday at 4:42 a.m. PT (7:42 a.m. ET), the U.S. Geological Survey reported. The quake left about 2,000 customers without power, said David Eisenhower, a spokesman for Pacific Gas and Light. Under the USGS classification, a magnitude 4.2 earthquake is considered \"light,\" which it says usually causes minimal damage. \"We had quite a spike in calls, mostly calls of inquiry, none of any injury, none of any damage that was reported,\" said Capt. Al Casciato of the San Francisco police. \"It was fairly mild.\" Watch police describe concerned calls immediately after the quake Â» . The quake was centered about two miles east-northeast of Oakland, at a depth of 3.6 miles, the USGS said. Oakland is just east of San Francisco, across San Francisco Bay. An Oakland police dispatcher told CNN the quake set off alarms at people's homes. The shaking lasted about 50 seconds, said CNN meteorologist Chad Myers. According to the USGS, magnitude 4.2 quakes are felt indoors and may break dishes and windows and overturn unstable objects. Pendulum clocks may stop.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9754257a-74ff-4747-a18e-7ebfbc82639c",
   "metadata": {
    "id": "9754257a-74ff-4747-a18e-7ebfbc82639c"
   },
   "source": [
    "**Exercise:**\n",
    "Summarise the story text using the following three prompts. Use the format given above but here there is no need to set the persona (i.e. only include one dictionary in the messages list when calling `client.chat.completions.create`.) Comment on any differences.\n",
    "\n",
    "1) \"Summarise the following article in 3 sentences.\"\n",
    "\n",
    "2) \"Give me a TL;DR of this text.\"\n",
    "\n",
    "3) \"What's the key takeaway here?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9c372155-bdde-45a4-a184-fce4c4e8034c",
   "metadata": {
    "id": "9c372155-bdde-45a4-a184-fce4c4e8034c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarise the following article in 3 sentences.  \n",
      " Here is a 3 sentence summary of the article:\n",
      "\n",
      "A magnitude 4.2 earthquake struck the San Francisco area early Friday morning, causing minimal damage and prompting about 2,000 power outages. The quake, which was centered about two miles east-northeast of Oakland, was described as \"light\" and left no reported injuries or significant damage. The shaking lasted about 50 seconds and triggered alarms at people's homes and may have caused objects to fall or break, as is typical for an earthquake of this magnitude.\n",
      "Give me a TL;DR of this text.  \n",
      " Here is a TL;DR of the text:\n",
      "\n",
      "A magnitude 4.2 earthquake struck the San Francisco area on Friday at 4:42am, leaving around 2,000 customers without power. The USGS classifies the quake as \"light\" and says it caused minimal damage, with no reported injuries or significant damage. The quake was centered near Oakland and lasted about 50 seconds.\n",
      "What's the key takeaway here? \n",
      " The key takeaway is that a magnitude 4.2 earthquake struck the San Francisco area, causing some power outages (about 2,000 customers affected) but no injuries or significant damage, according to officials and reports.\n"
     ]
    }
   ],
   "source": [
    "prompts = [\"Summarise the following article in 3 sentences. \", \"Give me a TL;DR of this text. \", \"What's the key takeaway here?\"]\n",
    "#content will be p + story for p in prompts\n",
    "\n",
    "# ANSWER\n",
    "for p in prompts:\n",
    "    response = client.chat.completions.create(\n",
    "                model=\"llama3-8b-8192\",\n",
    "                messages=[{\"role\": \"user\", \"content\": p + story}]\n",
    ")\n",
    "\n",
    "    print(p, '\\n', response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3634dd68-a9c1-42a2-a1ef-eb1487cbf9df",
   "metadata": {
    "id": "3634dd68-a9c1-42a2-a1ef-eb1487cbf9df"
   },
   "source": [
    "Run the above code again below and note that the answers may differ. This is due to the probabilistic nature of LLM token generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f7dffda3-fb3b-4555-b6c6-9bbc33248c14",
   "metadata": {
    "id": "f7dffda3-fb3b-4555-b6c6-9bbc33248c14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarise the following article in 3 sentences.  \n",
      " Here is a summary of the article in 3 sentences:\n",
      "\n",
      "A magnitude 4.2 earthquake struck the San Francisco area at 4:42 a.m. PT on Friday, causing minimal damage and leaving about 2,000 customers without power. The USGS classified the quake as \"light\" and reported that it was centered about two miles east-northeast of Oakland, at a depth of 3.6 miles. The shaking lasted about 50 seconds and caused some alarms to go off at people's homes, with no reports of injury or significant damage.\n",
      "Give me a TL;DR of this text.  \n",
      " Here is a brief summary of the text:\n",
      "\n",
      "A magnitude 4.2 earthquake struck the San Francisco area early Friday morning, causing about 2,000 customers to lose power. The quake was considered \"light\" and caused minimal damage, with no reports of injuries or significant damage. The quake was centered near Oakland and lasted for about 50 seconds, with some people reporting their home alarms went off.\n",
      "What's the key takeaway here? \n",
      " The key takeaway is that a magnitude 4.2 earthquake occurred in the San Francisco area at approximately 4:42 am, causing about 2,000 customers to lose power, but resulting in no reported injuries or significant damage.\n"
     ]
    }
   ],
   "source": [
    "# ANSWER\n",
    "for p in prompts:\n",
    "    response = client.chat.completions.create(\n",
    "                model=\"llama3-8b-8192\",\n",
    "                messages=[{\"role\": \"user\", \"content\": p + story}]\n",
    ")\n",
    "\n",
    "    print(p, '\\n', response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b4a80d-acb8-4099-b8e2-dba4a372369f",
   "metadata": {
    "id": "97b4a80d-acb8-4099-b8e2-dba4a372369f"
   },
   "source": [
    "## Text completion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0be4be-e054-4b9c-ad38-843c97d3afaf",
   "metadata": {
    "id": "2b0be4be-e054-4b9c-ad38-843c97d3afaf"
   },
   "source": [
    "**Exercise**: In this section adjust the `max_completion_tokens` and `temperature` settings below to obtain different responses. Show some examples with the prompt \"Continue the story: It was a great time to be alive\" with the model \"llama-3.1-8b-instant\".\n",
    "\n",
    "* max_completion_tokens - the maximum number of tokens to generate. Note that longer words are made of multiple tokens (set to 200 and 500)\n",
    "* temperature (positive number) - the higher the number the more random (creative) the output (set to 0.2, 0.8, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "50207704-94be-4309-8a82-dc1d22a063ee",
   "metadata": {
    "id": "50207704-94be-4309-8a82-dc1d22a063ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was a great time to be alive, and Emily couldn't help but feel a sense of excitement and possibility as she walked through the crowded streets of the city. The year was 1969, and the music scene was booming. The air was electric with the sounds of Jimi Hendrix, The Beatles, and Janis Joplin.\n",
      "\n",
      "Emily, a 20-year-old art student, was standing on the corner of 23rd Street and Broadway, surrounded by the pulsating rhythm of the city. She had just left a lively conversation with her friend, Sarah, about the latest happenings in the art world. They had spent hours discussing the latest movements - Abstract Expressionism, Pop Art, and the emergence of Conceptual Art.\n",
      "\n",
      "As Emily gazed out at the sea of faces passing by, she noticed a flyer taped to a streetlamp. It was an advertisement for the latest exhibit at the Museum of Modern Art (MoMA), featuring works by Warhol, Lichtenstein\n"
     ]
    }
   ],
   "source": [
    "# ANSWER (set max_completion_tokens=200, do not have a temperature setting)\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Continue the story: It was a great time to be alive\"}],\n",
    "    max_completion_tokens=200,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "63235fcc-3a61-4829-998b-37f2f104322b",
   "metadata": {
    "id": "63235fcc-3a61-4829-998b-37f2f104322b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was a great time to be alive. The summer sun was shining brightly, casting a warm glow over the vibrant city. The air was alive with the hum of music and the chatter of people from all walks of life. everywhere, neon lights seemed to dance in every possible color, and the smell of street food wafted through the air, teasing the senses.\n",
      "\n",
      "Lena, a young artist, was sitting on a bench, strumming a melancholy tune on her guitar. She had always found solace in music, and the rhythm seemed to match the beat of her own heart. As she played, she couldn't help but think about the world around her. It was a time of great change, a time of great uncertainty.\n",
      "\n",
      "But amidst all the chaos, there was a sense of hope. A sense that anything was possible, that the future was bright and full of promise. Lena's eyes met those of a stranger, a boy with piercing blue eyes and a mop of messy brown hair. He was walking by, his feet tapping out the beat of the music to Lena's guitar.\n",
      "\n",
      "For a moment, they locked eyes, and Lena felt a jolt of electricity run through her body. She quickly looked away, her cheeks flushing with embarrassment. But the boy didn't seem to notice, or maybe he was just too caught up in the music. He continued walking, but Lena couldn't shake the feeling that their eyes had met for a reason.\n",
      "\n",
      "As she played on, the music seemed to take on a new life. The notes seemed to swirl together in a joyful celebration of the world around her. And when the song finally came to an end, Lena felt a sense of satisfaction wash over her. She had created something beautiful, something that could touch the hearts of others.\n",
      "\n",
      "But as she packed up her guitar and stood up to leave, she couldn't help but wonder if their eyes had met by chance, or if there was something more to it. She caught a glimpse of the boy in the distance, and for a moment, their eyes met again. This time, he smiled, and Lena's heart skipped a beat.\n",
      "\n",
      "It was a small gesture, but it was enough to make Lena feel like she was exactly where she was meant to be. She smiled back, and the two of them seemed to share a secret understanding. Maybe it was just a spark, but for Lena, it was enough to make her feel alive.\n",
      "\n",
      "And as she walked away from the\n"
     ]
    }
   ],
   "source": [
    "# ANSWER (set max_completion_tokens=500, do not have a temperature setting)\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Continue the story: It was a great time to be alive\"}],\n",
    "    max_completion_tokens=500,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8ac91a1a-4f55-4531-bb5b-cdb436a87e50",
   "metadata": {
    "id": "8ac91a1a-4f55-4531-bb5b-cdb436a87e50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was a great time to be alive, and for Emily, it was especially true. The year was 1969, and the world was on the cusp of a revolution. Music, fashion, and art were all pushing the boundaries of what was considered acceptable, and Emily was right in the middle of it all.\n",
      "\n",
      "She had just turned 20, and her life was a whirlwind of concerts, protests, and late-night conversations about the meaning of life. She had just started dating a free-spirited artist named Max, who had a passion for painting and a penchant for getting into trouble.\n",
      "\n",
      "Together, they spent their days exploring the city, attending underground music shows, and talking about the latest issues of the day. The Vietnam War was raging, and the civil rights movement was gaining momentum. Emily and Max were both passionate about social justice, and they spent countless hours discussing the latest news and developments.\n",
      "\n",
      "One night, as they were walking home from a concert, they stumbled upon a group of people gathered in the park. They were listening to a young musician play a haunting melody on his guitar. Emily and Max sat down to listen, and as they did, they felt a sense of community and connection that they had never experienced before.\n",
      "\n",
      "The musician finished his song, and the crowd erupted into applause. As they were cheering, Emily turned to Max and said, \"This is it. This is the moment. We're living in a time of great change, and we're a part of it.\"\n",
      "\n",
      "Max smiled and took her hand. \"We're not just a part of it,\" he said. \"We're helping to create it.\"\n",
      "\n",
      "As they walked home, hand in hand, Emily felt a sense of excitement and possibility that she had never felt before. She knew that she was living in a great time, and she was grateful to be a part of it.\n",
      "\n",
      "But little did she know, the best was yet to come. The summer of '69 was just beginning, and it would be a summer that would change her life forever.\n",
      "\n",
      "---\n",
      "\n",
      "Would you like me to continue the story?\n"
     ]
    }
   ],
   "source": [
    "# ANSWER (set temperature = 0.2, do not have a max_completion_tokens setting)\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Continue the story: It was a great time to be alive\"}],\n",
    "    temperature = 0.2,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "27aef18a-eee4-4d30-9de9-f97a303ad78d",
   "metadata": {
    "id": "27aef18a-eee4-4d30-9de9-f97a303ad78d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sun was shining bright, casting a warm glow over the bustling streets of the city. People of all ages walked hand in hand, their faces filled with joy and their hearts full of hope. It was a great time to be alive, indeed.\n",
      "\n",
      "Lena, a young and ambitious journalist, sat at her desk, tapping away at her computer's keyboard as she worked on her latest article. She had just received a tip about a groundbreaking new development in the city's downtown area, and she was determined to get the exclusive scoop.\n",
      "\n",
      "As she typed away, the sounds of the city drifted in through the window: the chatter of pedestrians, the honking of cars, and the wail of sirens in the distance. Lena's editor, Rachel, poked her head into the room, a look of excitement on her face.\n",
      "\n",
      "\"Lena, I have some news,\" Rachel said, barely containing her enthusiasm. \"I just got word that the city council is going to announce a major new project tonight at a press conference.\"\n",
      "\n",
      "Lena's eyes lit up. \"What kind of project?\" she asked, her pen poised over her notebook.\n",
      "\n",
      "Rachel grinned mischievously. \"Let's just say it's going to change the face of our city forever.\"\n",
      "\n",
      "Lena's mind was racing. She had a feeling that this was the story of a lifetime. She quickly finished her article and rushed out the door, determined to get to the press conference and get the scoop.\n",
      "\n",
      "As she walked through the city, Lena couldn't help but feel a sense of optimism. Something was happening in this city, and she was determined to be at the forefront of it.\n",
      "\n",
      "She arrived at the press conference just as it was starting, a crowd of reporters and camera crews jostling for position in front of the city's mayor. Lena squeezed her way to the front, notebook and pen at the ready.\n",
      "\n",
      "The mayor stepped up to the microphone, a smile spreading across her face. \"Ladies and gentlemen, I am thrilled to announce that we are launching a new initiative to revitalize our city's downtown area,\" she began.\n",
      "\n",
      "The room erupted into a flurry of questions and camera clicks, but Lena was focused on getting the one question that would reveal the truth behind the announcement. She raised her hand, her voice cutting through the din.\n",
      "\n",
      "\"Mayor, can you tell us more about the specifics of the project?\" she asked, her pen scribbling furiously in her notebook as she waited for the answer.\n",
      "\n",
      "The mayor's smile grew even wider as she began to reveal the details. It was a plan to create a sustainable and vibrant hub of culture and innovation, a place where people could come together to learn, to create, and to connect.\n",
      "\n",
      "As Lena listened to the mayor's words, she felt her heart swell with excitement. This was it. This was the story of a lifetime, and she was honored to be a part of it.\n",
      "\n",
      "It was, indeed, a great time to be alive. The future was bright, and Lena couldn't wait to see what it held.\n"
     ]
    }
   ],
   "source": [
    "# ANSWER (set temperature = 1, do not have a max_completion_tokens setting)\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Continue the story: It was a great time to be alive.\"}],\n",
    "    temperature = 1,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac38e059-e23a-44eb-92fc-3fec9c7bdcdf",
   "metadata": {
    "id": "ac38e059-e23a-44eb-92fc-3fec9c7bdcdf"
   },
   "source": [
    "Note what happens when the temperature is set too high!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "931f8e00-928c-4218-8e74-8c56269fbfcb",
   "metadata": {
    "id": "931f8e00-928c-4218-8e74-8c56269fbfcb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was the end of the 20th century, and the world seemed to be filled with endless possibilities. The cold war had just ended, technological advancements were revolutionizing every facet of life, and a sense of hope and idealism hung in the air. For Emily, a brilliant and vibrant college student, life was just beginning.\n",
      "\n",
      "She had just turned 25, and along with her friends, Sarah and Rachel, they had embarked on a once-in-a-lifetime journey â a road trip across the American West. The car was loaded with snacks, maps, and their trusty music player. As they cruised down the California Highway in their battered-but-beloved Volkswagen Beetle named Bertha, Emily couldn't contain her excitement.\n",
      "\n",
      "\"You know, Rach, I always wanted to write something epic.\" Emily ex claimed, her eyes locked onto miles of golden desert stretching out across their windshield. Rach raised an brow and replied 'what you had in mind?'\n",
      "\n",
      "\"I was thinking, what we just experienced? It deserves to be written â not like some cheesy love song, but real stories. We should capture the freedom, camaraderie, and magic on paper.\" A sudden urge took control over her and without looking back at either Sarah/Rath for permission Emily starts driving further and deeper away from town and towards a mysterious place where she has seen on local brochure. \n",
      "\n",
      "This journey of adventure wasn't supposed to go unnoticed...\n"
     ]
    }
   ],
   "source": [
    "# ANSWER (set temperature = 2, do not have a max_completion_tokens setting)\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Continue the story: It was a great time to be alive.\"}],\n",
    "    temperature = 2,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e9bd9a-ad6b-409b-a751-023575868b6c",
   "metadata": {
    "id": "f4e9bd9a-ad6b-409b-a751-023575868b6c"
   },
   "source": [
    "### Zero-shot and one-short prompting for question-answering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6662f0a-c7e3-4235-972d-2771781a4e53",
   "metadata": {
    "id": "f6662f0a-c7e3-4235-972d-2771781a4e53"
   },
   "source": [
    "This section shows the impact of prompting on the response. Zero-shot prompting means we provide the prompt without any examples or additional context. Let us initially ask Mistral a question using no prompting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e2d48674-a9cc-4e5a-8035-afcc37a01dfc",
   "metadata": {
    "id": "e2d48674-a9cc-4e5a-8035-afcc37a01dfc"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Chemical reactions occur when two or more substances, called reactants, interact with each other to form new substances, called products. The interaction between the reactants can result in a physical change, such as a change in state (e.g., solid to liquid), or a chemical change, such as the formation of a new compound.\n",
       "\n",
       "Here are the general steps involved in a chemical reaction:\n",
       "\n",
       "1. **Collision**: The reactant molecules collide with each other, resulting in the formation of a temporary complex.\n",
       "2. **Reaction**: The reactant molecules interact with each other, leading to the breaking or forming of chemical bonds.\n",
       "3. **Formation of products**: The reactant molecules are converted into new products, which can be in the form of atoms, molecules, or ions.\n",
       "4. **Release of energy**: The reaction may release or absorb energy, depending on the type of reaction.\n",
       "\n",
       "Types of chemical reactions:\n",
       "\n",
       "1. **Synthesis reaction**: Two or more reactants combine to form a new compound. (e.g., 2H2 + O2 â 2H2O)\n",
       "2. **Decomposition reaction**: A single reactant breaks down into two or more products. (e.g., 2H2O â 2H2 + O2)\n",
       "3. **Single displacement reaction**: One element is displaced by another element in a compound. (e.g., Zn + CuSO4 â ZnSO4 + Cu)\n",
       "4. **Double displacement reaction**: Two compounds exchange partners to form two new compounds. (e.g., NaCl + AgNO3 â NaNO3 + AgCl)\n",
       "5. **Combustion reaction**: A substance reacts with oxygen to release heat and light. (e.g., CH4 + 2O2 â CO2 + 2H2O)\n",
       "6. **Redox reaction**: A reaction that involves the transfer of electrons between reactants. (e.g., Zn + CuSO4 â ZnSO4 + Cu)\n",
       "\n",
       "To predict how two chemicals react, you need to consider the following factors:\n",
       "\n",
       "1. **Chemical properties**: The properties of the reactants, such as their reactivity, electronegativity, and bond strength.\n",
       "2. **Chemical structure**: The molecular structure of the reactants, including the types of bonds and functional groups present.\n",
       "3. **Reaction conditions**: The temperature, pressure, and concentration of the reactants.\n",
       "4. **Catalysts**: The presence of catalysts, which can speed up or slow down the reaction.\n",
       "\n",
       "By considering these factors, you can predict the type of reaction that will occur between two chemicals and the products that will be formed."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"How do two chemicals react?\"}],\n",
    "    temperature = 0.8,\n",
    ")\n",
    "\n",
    "Markdown(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e7bac7-52f6-4779-a2a3-d45d29975f5d",
   "metadata": {
    "id": "c7e7bac7-52f6-4779-a2a3-d45d29975f5d"
   },
   "source": [
    "**Exercise:** Ask the same question but modify the prompt to return the answer to the same question in a simpler form (still using the llama-3.1-8b-instant model). Experiment with different prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "92250b60-9d68-4906-80b8-db78fc9c2ae6",
   "metadata": {
    "id": "92250b60-9d68-4906-80b8-db78fc9c2ae6"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Imagine you have a big box of LEGO blocks in different colors. Each color represents a different chemical. Now, let's say you have two LEGO blocks that are different colors, like red and blue.\n",
       "\n",
       "When you mix these two LEGO blocks together, something cool happens. The colors mix and create a new color, like purple. This is kind of like what happens when two chemicals react.\n",
       "\n",
       "Chemicals are like the LEGO blocks, and when they mix together, they change and create something new. Sometimes, this new thing can be useful, like creating medicine to help us feel better. Other times, it might not be so useful and can even be bad for us.\n",
       "\n",
       "When two chemicals react, they usually do a few things:\n",
       "\n",
       "1. They mix together and change into a new substance.\n",
       "2. They might make heat, like when you mix baking soda and vinegar and see a fizzing reaction.\n",
       "3. They might make light, like when you mix two special chemicals that glow in the dark.\n",
       "\n",
       "So, chemical reactions are like LEGO block mixing, but instead of making toys, they make new substances that can be really cool or really important."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ANSWER\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Answer the following question as though I am 10 years old. How do two chemicals react?\"}],\n",
    "    temperature = 0.8,\n",
    ")\n",
    "\n",
    "Markdown(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04676a11-57a7-4f9c-87e2-128559f0ffce",
   "metadata": {
    "id": "04676a11-57a7-4f9c-87e2-128559f0ffce"
   },
   "source": [
    "### One-shot prompting ###\n",
    "\n",
    "Next, note the dramatic change when we give the following template setting a new role and providing an English question followed by a French translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fcb1c8fa-d3cb-400d-ae0f-3bc8f2d1d473",
   "metadata": {
    "id": "fcb1c8fa-d3cb-400d-ae0f-3bc8f2d1d473"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment deux composÃ©s chimiques rÃ©agissent-ils?\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=[{\"role\": \"system\",\n",
    "             \"content\": \"You translate English to French.\"},\n",
    "              {\"role\": \"user\",\n",
    "               \"content\": \"What time is it?\"},\n",
    "               {\"role\": \"assistant\",\n",
    "               \"content\": \"Quelle heure est-il?\"},\n",
    "              {\"role\": \"user\",\n",
    "               \"content\": \"How do two chemicals react?\"}],\n",
    "    temperature = 0.8,\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1394e5-34b9-46b8-aea6-87a7862b7269",
   "metadata": {
    "id": "fc1394e5-34b9-46b8-aea6-87a7862b7269"
   },
   "source": [
    "### Few-shot prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e3b3d0-c365-4138-9be0-9d324bc34050",
   "metadata": {
    "id": "d5e3b3d0-c365-4138-9be0-9d324bc34050"
   },
   "source": [
    "Recall that since the text generation process outputs one token at a time, their outputs often need adjusting. This is where examples can help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "68c6131b-3b30-45b0-8dab-0dc547033b6b",
   "metadata": {
    "id": "68c6131b-3b30-45b0-8dab-0dc547033b6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I regret to inform you that I will be unable to attend the meeting.\n"
     ]
    }
   ],
   "source": [
    "prompt1 = \"I'm gonna head out now, see you later.\"\n",
    "response1 = \"I will be leaving now. See you later.\"\n",
    "\n",
    "prompt2 =  \"That movie was super cool!\"\n",
    "response2 = \"The movie was very impressive.\"\n",
    "\n",
    "prompt3 = \"Can't make it to the meeting, sorry.\"\n",
    "\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama3-70b-8192\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a professional editor. Rewrite casual sentences into a formal tone.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt1},\n",
    "        {\"role\": \"assistant\", \"content\": response1},\n",
    "        {\"role\": \"user\", \"content\": prompt2},\n",
    "        {\"role\": \"assistant\", \"content\": response2},\n",
    "        {\"role\": \"user\", \"content\": prompt3},\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content.strip())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040e9e18-cc27-453b-8844-3f683fb607f8",
   "metadata": {
    "id": "040e9e18-cc27-453b-8844-3f683fb607f8"
   },
   "source": [
    "The output can also be moulded to provide SQL output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ed854534-30db-4745-b910-27d12a3ed47e",
   "metadata": {
    "id": "ed854534-30db-4745-b910-27d12a3ed47e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT * FROM products WHERE stock_quantity = 0;\n"
     ]
    }
   ],
   "source": [
    "prompt1 = \"Show me all users who signed up in the last 30 days.\"\n",
    "response1 = \"SELECT * FROM users WHERE signup_date >= CURRENT_DATE - INTERVAL '30 days';\"\n",
    "\n",
    "prompt2 = \"What is the average order value?\"\n",
    "response2 =  \"SELECT AVG(order_total) FROM orders;\"\n",
    "\n",
    "prompt3 = \"List products that are out of stock.\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama3-70b-8192\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are an assistant that translates natural language to SQL.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt1},\n",
    "        {\"role\": \"assistant\", \"content\": response1},\n",
    "        {\"role\": \"user\", \"content\": prompt2},\n",
    "        {\"role\": \"assistant\", \"content\": response2},\n",
    "        {\"role\": \"user\", \"content\": prompt3},\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content.strip())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a1dda2-2913-4a59-bfc3-3f0a59f0dcc7",
   "metadata": {
    "id": "45a1dda2-2913-4a59-bfc3-3f0a59f0dcc7"
   },
   "source": [
    "**Exercise**: Create a few examples to train the \"llama3-70b-8192\" LLM to take in user content in the form below and provide output as a pandas dataframe. Use the `exec` function to execute its output to display the answer of sample input as a data frame.\n",
    "\n",
    "Example:\n",
    "\n",
    "given the user content\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "| col1 | col2 | col3\n",
    "\n",
    "| 32 | 27 | 25\n",
    "\n",
    "| 64 | 23 | 14\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "train the model to output\n",
    "\n",
    "df = pd.DataFrame({'col1': [32, 64], 'col2': [27, 23], 'col3': [25, 14]})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2b8ac08a-5759-41cd-a560-e77694573723",
   "metadata": {
    "id": "2b8ac08a-5759-41cd-a560-e77694573723"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>colA</th>\n",
       "      <th>colB</th>\n",
       "      <th>colC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23</td>\n",
       "      <td>12</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>76</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   colA  colB  colC\n",
       "0    23    12    54\n",
       "1     8    76    32\n",
       "2     7     5     3"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ANSWER\n",
    "user1 = \"\"\"col1 | col2 | col3\n",
    "32 | 27 | 25\n",
    "64 | 23 | 14\n",
    "\"\"\"\n",
    "\n",
    "output1 = \"\"\"\n",
    "df = pd.DataFrame({'col1': [32, 64], 'col2': [27, 23], 'col3': [25, 14]})\n",
    "\"\"\"\n",
    "\n",
    "user2 = \"\"\"col1 | col2\n",
    "23 | 12\n",
    "8 | 76\n",
    "7 | 5\n",
    "\"\"\"\n",
    "output2 = \"\"\"\n",
    "df = pd.DataFrame({'col1': [23, 8, 7], 'col2': [12, 76, 5]})\n",
    "\"\"\"\n",
    "user3 = \"\"\"colA | colB | colC\n",
    "23 | 12 | 54\n",
    "8 | 76 | 32\n",
    "7 | 5 | 3\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama3-70b-8192\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a data scientist who will receive data input as a string and provide output as a pandas dataframe called df. Use the examples to guide you\"},\n",
    "        {\"role\": \"user\", \"content\": user1},\n",
    "        {\"role\": \"assistant\", \"content\": output1},\n",
    "        {\"role\": \"user\", \"content\": user2},\n",
    "        {\"role\": \"user\", \"content\": output2},\n",
    "        {\"role\": \"user\", \"content\": user3}\n",
    "    ]\n",
    ")\n",
    "\n",
    "exec(response.choices[0].message.content.strip()) # string executed as Python code\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f62a88-c7b4-4431-acda-97e9a98981f4",
   "metadata": {
    "id": "29f62a88-c7b4-4431-acda-97e9a98981f4"
   },
   "source": [
    "Also show what happens when the question is asked in the absence of a system role and without few-shot prompting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "712221ea-14c0-4faa-a126-ca4f2f0538a6",
   "metadata": {
    "id": "712221ea-14c0-4faa-a126-ca4f2f0538a6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"It looks like you've provided a table with three columns: colA, colB, and colC. Here's a formatted version of the table:\\n\\n| colA | colB | colC |\\n| --- | --- | --- |\\n| 23 | 12 | 54 |\\n| 8 | 76 | 32 |\\n| 7 | 5 | 3 |\\n\\nWhat would you like to do with this table?\""
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ANSWER\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama3-70b-8192\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user3}\n",
    "    ]\n",
    ")\n",
    "response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28660884-dfd4-4b96-a65d-d21ddbf99423",
   "metadata": {
    "id": "28660884-dfd4-4b96-a65d-d21ddbf99423"
   },
   "source": [
    "### Chain-of-thought prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724f85ef-afc0-4e31-9ad8-ae780c535837",
   "metadata": {
    "id": "724f85ef-afc0-4e31-9ad8-ae780c535837"
   },
   "source": [
    "The results of question-answering can also be improved by prompting the LLM to provide intermediate steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee04ad6-968b-406a-8119-981b01ef5f92",
   "metadata": {
    "id": "bee04ad6-968b-406a-8119-981b01ef5f92"
   },
   "source": [
    "**Exercise**: Using the following prompts, compare the answers of the \"llama3-8b-8192\" model (set seed=21). (If this model is no longer available choose a model with relatively few parameters.)\n",
    "\n",
    "zero_shot_prompt = \"How many s's are in the word 'success'?\"\n",
    "\n",
    "chain_of_thought_prompt = \"How many s's are in the word 'success'? Explain your answer step by step by going through each letter in turn.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "bc8868f2-7c4f-4b99-bbd2-cc731cca74bb",
   "metadata": {
    "id": "bc8868f2-7c4f-4b99-bbd2-cc731cca74bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------zero-shot-prompt------\n",
      "There are 2 s's in the word 'success'.\n",
      "------chain-of-thought------\n",
      "To count the number of 's's in the word \"success\", we'll go through each letter one by one:\n",
      "\n",
      "1. S\n",
      "   There's one 's' in the first position.\n",
      "\n",
      "2. U\n",
      "   Nothing else is changing here as this is just a 'U'.\n",
      "\n",
      "3. C\n",
      "   We still have the same number of s's, 1.\n",
      "\n",
      "4. C\n",
      "   The 's' number remains 1.\n",
      "\n",
      "5. E\n",
      "   Still at 1 s'.\n",
      "\n",
      "6. S\n",
      "   There's now a second 's'.\n",
      "\n",
      "7. S\n",
      "   There's now a third 's'\n",
      "\n",
      "In the end, there are 3 's's in the word \"success\".\n"
     ]
    }
   ],
   "source": [
    "# ANSWER\n",
    "zero_shot_prompt = \"How many s's are in the word 'success'?\"\n",
    "chain_of_thought_prompt = \"How many s's are in the word 'success'? Explain your answer step by step by going through each letter in turn.\"\n",
    "\n",
    "response1 = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=[{\"role\": \"user\", \"content\": zero_shot_prompt}],\n",
    "    seed = 21\n",
    ")\n",
    "\n",
    "response2 = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=[{\"role\": \"user\", \"content\": chain_of_thought_prompt}],\n",
    "    seed = 21\n",
    ")\n",
    "\n",
    "print('------zero-shot-prompt------')\n",
    "print(response1.choices[0].message.content)\n",
    "\n",
    "print('------chain-of-thought------')\n",
    "print(response2.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09499b6e-a4aa-439c-a785-3f0e06a3ba4f",
   "metadata": {
    "id": "09499b6e-a4aa-439c-a785-3f0e06a3ba4f"
   },
   "source": [
    "## Comparison of LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0e82d9-cefc-4af3-9d95-5e22db6cd56f",
   "metadata": {
    "id": "dd0e82d9-cefc-4af3-9d95-5e22db6cd56f"
   },
   "source": [
    "**Exercise**: Compare the performance of 2 LLMs by outputting the answers of the following questions into a dataframe.\n",
    "\n",
    "    \"Tell me a joke about data science.\",\n",
    "    \"How can one calculate 22 * 13 mentally?\",\n",
    "    \"Write a creative story about a baby learning to crawl.\",\n",
    "\n",
    "Column headings:\n",
    "\n",
    "Model Name | Question | Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b5e0c587-9650-4082-baf4-d9cdd94bc238",
   "metadata": {
    "id": "b5e0c587-9650-4082-baf4-d9cdd94bc238"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model Name</th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gemma2-9b-it</td>\n",
       "      <td>Tell me a joke about data science.</td>\n",
       "      <td>Why did the data scientist break up with the statistician? \\n\\nBecause they had too many significant differences!  ð  \\n\\n\\nLet me know if you want to hear another one! ð</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gemma2-9b-it</td>\n",
       "      <td>How can one calculate 22 * 13 mentally?</td>\n",
       "      <td>Here's a trick to calculate 22 * 13 mentally:\\n\\n**Break it down:**\\n\\n*  **Think of 22 as (20 + 2)**\\n*  **Multiply each part by 13:**\\n    * 20 * 13 = 260\\n    * 2 * 13 = 26\\n\\n*  **Add the two results:** 260 + 26 = 286\\n\\n\\nTherefore, 22 * 13 = 286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gemma2-9b-it</td>\n",
       "      <td>Write a creative story about a baby learning to crawl.</td>\n",
       "      <td>Bertie, a fuzzy-headed cherub with eyes like melted chocolate, gazed longingly at the towering figure of his teddy bear, Mr. Snugglesworth. Mr. Snugglesworth sat atop his playmat, a smug, cotton smile stitched onto his furry face. Bertie wanted to reach him, desperately.\\n\\nHis attempts were...exasperating. Heâd flail his knobby legs, kick a wayward foot across the room, and roll with the fury of a tiny hurricane. He could scoot, he could turn, he could even sit up with a wobbly grin and clap his chubby hands, but Mr. Snugglesworth remained frustratingly out of reach.\\n\\nThen, one sunny morning, something shifted. Bertie, inspired by his reflection in the shiny mobile above him, decided to try something new. He saw his chubby arms reach forward, mirroring the jerky motion of the dangling baubles.\\n\\nHe pushed himself up, his fingers digging into the plush carpet. The room tilted, his vision blurring for a moment, but he steadied himself. His legs wobbled, but he held on. He took a tentative step, a leg inching forward, then another. His grin widened, transforming his face into a permanent open-mouthed expression.\\n\\nHe moved like a wobbling tank, propelled by sheer determination. Every inch gained was a victory. He bumped into his toys, fell on his face, rolled onto his back, and giggled uncontrollably.\\n\\nHe was crawling, even if it was more like an ungainly ballet performed by a fuzzy, leggy disaster. But it was crawling, nonetheless.\\n\\nBertie, spurred on by his accomplishment, crawled towards his nemesis: Mr. Snugglesworth. Each inch closer was met with squeals of delight. Finally, his tiny hands reached the bearâs soft fur.\\n\\nHe collapsed on top of Mr. Snugglesworth, burying his face in the bear's cotton cheek. Mr. Snugglesworth, silent in his plush dignity, seemed to wink his sewn-on eye. Bertie had conquered the world, or at least, his playmat â and for a baby, that was enough.\\n\\n\\nHeâd learned that day, the most valuable lesson a baby can learn: even the most daunting mountains can be climbed, one wobbly step at a time.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>llama-3.1-8b-instant</td>\n",
       "      <td>Tell me a joke about data science.</td>\n",
       "      <td>Why did the data scientist bring a ladder to the party?\\n\\nBecause they heard the drinks were on the house.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>llama-3.1-8b-instant</td>\n",
       "      <td>How can one calculate 22 * 13 mentally?</td>\n",
       "      <td>To calculate 22 * 13 mentally, you can use the following method:\\n\\n1. Break down the numbers into smaller parts. In this case, 13 can be broken down into 10 + 3.\\n2. Calculate 22 * 10: 220 (since 20 * 10 = 200 and adding 2 to it gives you 220)\\n3. Now, calculate 22 * 3: 66\\n4. Add the two results: 220 + 66 = 286\\n\\nSo, the answer is 286.\\n\\nAnother method is:\\n\\n1. Multiply 22 by a nearby round number, 20, which gives you 400, and multiply by 2 (as in 22).\\n2. Calculate 22 x 0.1 * 3 = 6.6\\n3. Then add 6.6 to 400 to get your solution which is 400 + 66 =  286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>llama-3.1-8b-instant</td>\n",
       "      <td>Write a creative story about a baby learning to crawl.</td>\n",
       "      <td>Once upon a time, in a cozy little home surrounded by lush green plants and warm sunshine, there lived a baby named Luna. Luna was a bundle of energy and curiosity, always eager to explore the world around her. Her bright blue eyes sparkled with excitement as she discovered new sights, sounds, and smells every day.\\n\\nAs Luna grew bigger and stronger, she began to feel the urge to move beyond the confines of her crib. She watched with envy as her big brother, Finn, effortlessly crawled across the floor, his bright smile and twinkling laughter captivating everyone's attention.\\n\\nLuna's determination was contagious, and her parents, Emily and Ryan, couldn't resist her pleading gaze. They patiently encouraged her to practice crawling, placing toys and other enticing objects just out of reach to motivate her.\\n\\nAt first, Luna tried to imitate Finn's movements, but her little body wobbled and shook as she struggled to coordinate her arms and legs. She flailed her tiny hands and kicked her feet, but it was more like a wobbly dance than actual crawling.\\n\\nEmily and Ryan cheered her on, offering reassurance and support whenever Luna faltered. They reminded her that learning to crawl takes time, patience, and practice, and encouraged her to keep trying.\\n\\nOne sunny afternoon, Luna decided to give it another go. She crawled out of her crib and onto the soft, plush carpet, her eyes fixed on a vibrant red ball sitting just a few inches away. With a determined glint in her eye, she began to push herself forward, her hands and feet working in tandem as she inched closer to her goal.\\n\\nAt first, progress was slow, and Luna's face scrunched up in concentration. She slid backwards, her tiny hands scrambling to regain her balance. But she refused to give up. Instead, she picked herself up, dusted herself off, and tried again.\\n\\nSlowly but surely, Luna gained momentum, her confidence growing with each tiny success. She crawled a little farther, then a little more, until finally, she reached the red ball. With a triumphant squeal, Luna grasped the ball in her fist and held it aloft, her face beaming with pride.\\n\\nEmily and Ryan cheered and hugged Luna, tears of joy welling up in their eyes. \"You did it, little one!\" they exclaimed, beaming with pride.\\n\\nFrom that moment on, Luna became a seasoned crawler, zooming across the floor with ease and giggling with joy. She explored every nook and cranny, discovering new wonders and making memories that would last a lifetime.\\n\\nAs she grew and learned, Luna's parents watched with pride, grateful for the little miracle that had brought such joy and love into their lives. And whenever they looked at their little crawler, they knew that they had played a small part in nurturing her curiosity, her determination, and her incredible spirit.\\n\\nYears later, when Luna looked back on her journey, she would remember the day she learned to crawl as a turning point in her life. It was the moment when she discovered the thrill of exploration, the joy of learning, and the confidence that came with trying new things.\\n\\nAnd as she gazed out at the world, her eyes shining bright with wonder, Luna knew that she was ready for whatever lay ahead â a world full of possibilities, laughter, and endless exploration.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Model Name  \\\n",
       "0          gemma2-9b-it   \n",
       "1          gemma2-9b-it   \n",
       "2          gemma2-9b-it   \n",
       "3  llama-3.1-8b-instant   \n",
       "4  llama-3.1-8b-instant   \n",
       "5  llama-3.1-8b-instant   \n",
       "\n",
       "                                                 Question  \\\n",
       "0                      Tell me a joke about data science.   \n",
       "1                 How can one calculate 22 * 13 mentally?   \n",
       "2  Write a creative story about a baby learning to crawl.   \n",
       "3                      Tell me a joke about data science.   \n",
       "4                 How can one calculate 22 * 13 mentally?   \n",
       "5  Write a creative story about a baby learning to crawl.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Answer  \n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Why did the data scientist break up with the statistician? \\n\\nBecause they had too many significant differences!  ð  \\n\\n\\nLet me know if you want to hear another one! ð  \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Here's a trick to calculate 22 * 13 mentally:\\n\\n**Break it down:**\\n\\n*  **Think of 22 as (20 + 2)**\\n*  **Multiply each part by 13:**\\n    * 20 * 13 = 260\\n    * 2 * 13 = 26\\n\\n*  **Add the two results:** 260 + 26 = 286\\n\\n\\nTherefore, 22 * 13 = 286  \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Bertie, a fuzzy-headed cherub with eyes like melted chocolate, gazed longingly at the towering figure of his teddy bear, Mr. Snugglesworth. Mr. Snugglesworth sat atop his playmat, a smug, cotton smile stitched onto his furry face. Bertie wanted to reach him, desperately.\\n\\nHis attempts were...exasperating. Heâd flail his knobby legs, kick a wayward foot across the room, and roll with the fury of a tiny hurricane. He could scoot, he could turn, he could even sit up with a wobbly grin and clap his chubby hands, but Mr. Snugglesworth remained frustratingly out of reach.\\n\\nThen, one sunny morning, something shifted. Bertie, inspired by his reflection in the shiny mobile above him, decided to try something new. He saw his chubby arms reach forward, mirroring the jerky motion of the dangling baubles.\\n\\nHe pushed himself up, his fingers digging into the plush carpet. The room tilted, his vision blurring for a moment, but he steadied himself. His legs wobbled, but he held on. He took a tentative step, a leg inching forward, then another. His grin widened, transforming his face into a permanent open-mouthed expression.\\n\\nHe moved like a wobbling tank, propelled by sheer determination. Every inch gained was a victory. He bumped into his toys, fell on his face, rolled onto his back, and giggled uncontrollably.\\n\\nHe was crawling, even if it was more like an ungainly ballet performed by a fuzzy, leggy disaster. But it was crawling, nonetheless.\\n\\nBertie, spurred on by his accomplishment, crawled towards his nemesis: Mr. Snugglesworth. Each inch closer was met with squeals of delight. Finally, his tiny hands reached the bearâs soft fur.\\n\\nHe collapsed on top of Mr. Snugglesworth, burying his face in the bear's cotton cheek. Mr. Snugglesworth, silent in his plush dignity, seemed to wink his sewn-on eye. Bertie had conquered the world, or at least, his playmat â and for a baby, that was enough.\\n\\n\\nHeâd learned that day, the most valuable lesson a baby can learn: even the most daunting mountains can be climbed, one wobbly step at a time.  \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Why did the data scientist bring a ladder to the party?\\n\\nBecause they heard the drinks were on the house.  \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  To calculate 22 * 13 mentally, you can use the following method:\\n\\n1. Break down the numbers into smaller parts. In this case, 13 can be broken down into 10 + 3.\\n2. Calculate 22 * 10: 220 (since 20 * 10 = 200 and adding 2 to it gives you 220)\\n3. Now, calculate 22 * 3: 66\\n4. Add the two results: 220 + 66 = 286\\n\\nSo, the answer is 286.\\n\\nAnother method is:\\n\\n1. Multiply 22 by a nearby round number, 20, which gives you 400, and multiply by 2 (as in 22).\\n2. Calculate 22 x 0.1 * 3 = 6.6\\n3. Then add 6.6 to 400 to get your solution which is 400 + 66 =  286  \n",
       "5  Once upon a time, in a cozy little home surrounded by lush green plants and warm sunshine, there lived a baby named Luna. Luna was a bundle of energy and curiosity, always eager to explore the world around her. Her bright blue eyes sparkled with excitement as she discovered new sights, sounds, and smells every day.\\n\\nAs Luna grew bigger and stronger, she began to feel the urge to move beyond the confines of her crib. She watched with envy as her big brother, Finn, effortlessly crawled across the floor, his bright smile and twinkling laughter captivating everyone's attention.\\n\\nLuna's determination was contagious, and her parents, Emily and Ryan, couldn't resist her pleading gaze. They patiently encouraged her to practice crawling, placing toys and other enticing objects just out of reach to motivate her.\\n\\nAt first, Luna tried to imitate Finn's movements, but her little body wobbled and shook as she struggled to coordinate her arms and legs. She flailed her tiny hands and kicked her feet, but it was more like a wobbly dance than actual crawling.\\n\\nEmily and Ryan cheered her on, offering reassurance and support whenever Luna faltered. They reminded her that learning to crawl takes time, patience, and practice, and encouraged her to keep trying.\\n\\nOne sunny afternoon, Luna decided to give it another go. She crawled out of her crib and onto the soft, plush carpet, her eyes fixed on a vibrant red ball sitting just a few inches away. With a determined glint in her eye, she began to push herself forward, her hands and feet working in tandem as she inched closer to her goal.\\n\\nAt first, progress was slow, and Luna's face scrunched up in concentration. She slid backwards, her tiny hands scrambling to regain her balance. But she refused to give up. Instead, she picked herself up, dusted herself off, and tried again.\\n\\nSlowly but surely, Luna gained momentum, her confidence growing with each tiny success. She crawled a little farther, then a little more, until finally, she reached the red ball. With a triumphant squeal, Luna grasped the ball in her fist and held it aloft, her face beaming with pride.\\n\\nEmily and Ryan cheered and hugged Luna, tears of joy welling up in their eyes. \"You did it, little one!\" they exclaimed, beaming with pride.\\n\\nFrom that moment on, Luna became a seasoned crawler, zooming across the floor with ease and giggling with joy. She explored every nook and cranny, discovering new wonders and making memories that would last a lifetime.\\n\\nAs she grew and learned, Luna's parents watched with pride, grateful for the little miracle that had brought such joy and love into their lives. And whenever they looked at their little crawler, they knew that they had played a small part in nurturing her curiosity, her determination, and her incredible spirit.\\n\\nYears later, when Luna looked back on her journey, she would remember the day she learned to crawl as a turning point in her life. It was the moment when she discovered the thrill of exploration, the joy of learning, and the confidence that came with trying new things.\\n\\nAnd as she gazed out at the world, her eyes shining bright with wonder, Luna knew that she was ready for whatever lay ahead â a world full of possibilities, laughter, and endless exploration.  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None) # allows wide dataframes to be viewed\n",
    "models = [\"gemma2-9b-it\", \"llama-3.1-8b-instant\"] #can edit this\n",
    "\n",
    "# ANSWER\n",
    "prompts = [\n",
    "    \"Tell me a joke about data science.\",\n",
    "    \"How can one calculate 22 * 13 mentally?\",\n",
    "    \"Write a creative story about a baby learning to crawl.\",\n",
    "]\n",
    "\n",
    "results = {'Model Name': [], 'Question': [], 'Answer': []}\n",
    "\n",
    "for model in models:\n",
    "    for prompt in prompts:\n",
    "        results['Model Name'].append(model)\n",
    "        results['Question'].append(prompt)\n",
    "        try:\n",
    "            output = client.chat.completions.create(model = model, messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "            results['Answer'].append(output.choices[0].message.content.strip())\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error with {model}: {e}\")\n",
    "            results['Answer'].append((prompt, \"ERROR\"))\n",
    "\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3e2651-0bee-40bd-b5cc-3c2891b1adb5",
   "metadata": {
    "id": "8f3e2651-0bee-40bd-b5cc-3c2891b1adb5"
   },
   "source": [
    "### Bonus\n",
    "\n",
    "See if you can prompt an LLM to perform sentiment analysis (output 'Positive' or 'Negative' only) on a given piece of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d7169fd6-f1d1-4a5d-8fd9-21167a54416a",
   "metadata": {
    "id": "d7169fd6-f1d1-4a5d-8fd9-21167a54416a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Positive'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ANSWER\n",
    "input1 = \"I absolutely loved the way the story unfolded.\"\n",
    "output1 = \"Positive\"\n",
    "\n",
    "input2 = \"The food was cold and completely flavorless.\"\n",
    "output2 = \"Negative\"\n",
    "\n",
    "input3 = \"She handled the situation with grace and professionalism.\"\n",
    "\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama3-70b-8192\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are amazing at sentiment analysis. Give the sentiment of the next sentence as the examples show.\"},\n",
    "        {\"role\": \"user\", \"content\": input1},\n",
    "        {\"role\": \"assistant\", \"content\": output1},\n",
    "        {\"role\": \"user\", \"content\": input2},\n",
    "        {\"role\": \"assistant\", \"content\": output2},\n",
    "        {\"role\": \"user\", \"content\": input3},\n",
    "    ]\n",
    ")\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0387259e-0bf8-400e-97d9-d3f5688b2e4d",
   "metadata": {
    "id": "0387259e-0bf8-400e-97d9-d3f5688b2e4d"
   },
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f279421d-cbca-4003-941e-c2d2bc2833a8",
   "metadata": {
    "id": "f279421d-cbca-4003-941e-c2d2bc2833a8"
   },
   "source": [
    "We worked with a few Large Language Models (LLMs) using Groq and experimented with prompting for summarisation, text completion and question-answering tasks.\n",
    "\n",
    "We also explored controlling the randomness (creativity) of output through the temperature setting and tried different types of prompting to achieve desired forms of output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f06d5e-7073-485b-b046-afe839ab844c",
   "metadata": {
    "id": "94f06d5e-7073-485b-b046-afe839ab844c"
   },
   "source": [
    "## References\n",
    "1. [Groq's prompting guide](https://console.groq.com/docs/prompting)\n",
    "2. [Groq's playground](https://console.groq.com/playground)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61aab4a-1330-4762-9318-5635c3a97aa7",
   "metadata": {
    "id": "d61aab4a-1330-4762-9318-5635c3a97aa7"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "> > > > > > > > > Â© 2025 Institute of Data\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
